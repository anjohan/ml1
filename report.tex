\documentclass[11pt,british,a4paper]{article}
%\pdfobjcompresslevel=0
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[includeheadfoot,margin=0.8 in]{geometry}
\usepackage{siunitx,physics,cancel,upgreek,varioref,listings,booktabs,pdfpages,ifthen,polynom,todonotes}
%\usepackage{minted}
\usepackage[backend=biber]{biblatex}
\DefineBibliographyStrings{english}{%
      bibliography = {References},
}
\addbibresource{sources.bib}
\usepackage{mathtools,upgreek,bigints}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[T1]{fontenc}
%\usepackage{fouriernc}
% \usepackage[T1]{fontenc}
\usepackage{mathpazo}
% \usepackage{inconsolata}
%\usepackage{eulervm}
%\usepackage{cmbright}
%\usepackage{fontspec}
%\usepackage{unicode-math}
%\setmainfont{Tex Gyre Pagella}
%\setmathfont{Tex Gyre Pagella Math}
%\setmonofont{Tex Gyre Cursor}
%\renewcommand*\ttdefault{txtt}
\graphicspath{{figs/}}
\usepackage[scaled]{beramono}
\usepackage{fancyhdr}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{lastpage}
\usepackage{microtype}
\usepackage[font=normalsize]{subcaption}
\usepackage{luacode}
\usepackage[linktoc=all, bookmarks=true, pdfauthor={Anders Johansson},pdftitle={FYS-STK4155 Project 1}]{hyperref}
\usepackage{tikz,pgfplots,pgfplotstable}
\usepgfplotslibrary{colorbrewer}
\usepgfplotslibrary{external}
\tikzset{external/system call={lualatex \tikzexternalcheckshellescape -halt-on-error -interaction=batchmode -jobname "\image" "\texsource"}}
\tikzexternalize[prefix=tmp/, mode=list and make]
\pgfplotsset{cycle list/Dark2}
\pgfplotsset{compat=1.8}
\renewcommand{\CancelColor}{\color{red}}
\let\oldexp=\exp
\renewcommand{\exp}[1]{\mathrm{e}^{#1}}
\renewcommand{\Re}[1]{\mathfrak{Re}\ifthenelse{\equal{#1}{}}{}{\left(#1\right)}}
\renewcommand{\Im}[1]{\mathfrak{Im}\ifthenelse{\equal{#1}{}}{}{\left(#1\right)}}
\renewcommand{\i}{\mathrm{i}}
\newcommand{\tittel}[1]{\title{#1 \vspace{-7ex}}\author{}\date{}\maketitle\thispagestyle{fancy}\pagestyle{fancy}\setcounter{page}{1}}


\labelformat{section}{#1}
\labelformat{subsection}{exercise~#1}
\labelformat{subsubsection}{paragraph~#1}
\labelformat{equation}{equation~(#1)}
\labelformat{figure}{figure~#1}
\labelformat{table}{table~#1}

\renewcommand{\footrulewidth}{\headrulewidth}

%\setcounter{secnumdepth}{4}
\setlength{\parindent}{0cm}
\setlength{\parskip}{1em}

\definecolor{bluekeywords}{rgb}{0.13,0.13,1}
\definecolor{greencomments}{rgb}{0,0.5,0}
\definecolor{redstrings}{rgb}{0.9,0,0}
\lstset{rangeprefix=!/,
    rangesuffix=/!,
    includerangemarker=false}
\lstset{showstringspaces=false,
    basicstyle=\small\ttfamily,
    keywordstyle=\color{bluekeywords},
    commentstyle=\color{greencomments},
    numberstyle=\color{bluekeywords},
    stringstyle=\color{redstrings},
    breaklines=true,
    %texcl=true,
    language=Fortran,
    morekeywords={norm2}
}
\colorlet{DarkGrey}{white!20!black}
\newcommand{\eqtag}[1]{\refstepcounter{equation}\tag{\theequation}\label{#1}}
\hypersetup{hidelinks=True}

\sisetup{detect-all}
\sisetup{exponent-product = \cdot, output-product = \cdot,per-mode=symbol}
% \sisetup{output-decimal-marker={,}}
\sisetup{round-mode = off, round-precision=3}
\sisetup{number-unit-product = \ }

\allowdisplaybreaks[4]
\fancyhf{}

\rhead{Project 1}
\rfoot{Page~\thepage{} of~\pageref{LastPage}}
\lhead{FYS-STK4155}

%\definecolor{gronn}{rgb}{0.29, 0.33, 0.13}
\definecolor{gronn}{rgb}{0, 0.5, 0}

\newcommand{\husk}[2]{\tikz[baseline,remember picture,inner sep=0pt,outer sep=0pt]{\node[anchor=base] (#1) {\(#2\)};}}
\newcommand{\artanh}[1]{\operatorname{artanh}{\qty(#1)}}
\newcommand{\matrise}[1]{\begin{pmatrix}#1\end{pmatrix}}
\DeclareMathOperator{\Proj}{Proj}
\DeclareMathOperator{\Col}{Col}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\MSE}{MSE}

\newread\infile

\setcounter{tocdepth}{2}
\numberwithin{equation}{section}

%start
\begin{document}
\title{FYS-STK4155: Project 1}
\author{Anders Johansson}
%\maketitle

\begin{titlepage}
%\includegraphics[width=\textwidth]{fysisk.pdf}
\vspace*{\fill}
\begin{center}
\textsf{
    \Huge \textbf{Project 1}\\\vspace{0.5cm}
    \Large \textbf{FYS-STK4155 --- Applied data analysis and machine learning}\\
    \vspace{8cm}
    Anders Johansson\\
    \today\\
}
\vspace{1.5cm}
\includegraphics{uio.pdf}\\
\vspace*{\fill}
\end{center}
\end{titlepage}
\null
\pagestyle{empty}
\newpage

\pagestyle{fancy}
\setcounter{page}{1}

\begin{abstract}
    This project uses fitting of Franke's function and geographical data to compare ordinary least squares, Ridge and LASSO regression. Resampling is done in order to get accurate estimates for variances and means.
\end{abstract}

All files for this project are available at \url{https://github.com/anjohan/ml1}.

\tableofcontents

%  _       _
% (_)_ __ | |_ _ __ ___
% | | '_ \| __| '__/ _ \
% | | | | | |_| | | (_) |
% |_|_| |_|\__|_|  \___/
\section{Introduction}
Fitting of data is an important tool in most sciences.
The goal can be to interpolate between already measured data or predict values outside the measured range when extra experiments are infeasible, or to discover relations between quantities.
By assuming that the data points can be modelled by a linear combination of some set of functions and minimising the error, one ends up with linear regression.
This report explores three different linear regression methods --- ordinary least squares, which simply minimises the error, and Ridge and LASSO, which make up for their worse performance on training data by returning better predictors.

The main goal of this project is to compare these three regression methods empirically by applying it to two sets of data.
First, the regression methods are applied to a data set generated from a known function, specifically the Franke's function, which is a sum of bivariate Gaussian functions.
Having verified the implementation of the methods, they are then applied to geographic data, namely the altitude as a function of geographic coordinates.

The first part of this report goes through the basic theory of these regression methods and their implementation.
Then, the regression methods are applied to the Franke's function and the results are analysed with resampling. Finally, geographical data is fitted with all three methods.

%  _   _
% | |_| |__   ___  ___  _ __ _   _
% | __| '_ \ / _ \/ _ \| '__| | | |
% | |_| | | |  __/ (_) | |  | |_| |
%  \__|_| |_|\___|\___/|_|   \__, |
%                            |___/
\section{Theory and methods}

\subsection{Test case}
The function to be fitted as verification of the regression methods is the famous Franke's function \(f:\qty[0,1]^2\to\mathbb{R}\) given by
\begin{equation}
    \begin{alignedat}{2}
        f(x_1,x_2) &= \frac{3}{4}\oldexp(-\frac{(9x_1-2)^2}{4} - \frac{(9x_2-2)^2}{4})
                + \frac{3}{4}\oldexp(-\frac{(9x_1+1)^2}{49}- \frac{(9x_2+1)}{10} ) \\
               &+ \frac{1}{2}\oldexp(-\frac{(9x_1-7)^2}{4} - \frac{(9x_2-3)^2}{4})
                - \frac{1}{5}\oldexp(-(9x_1-4)^2 - (9x_2-7)^2)
    \end{alignedat}\label{eq:franke}
\end{equation}
and shown in~\vref{fig:franke}.

\begin{figure}[H]
    \centering
    \includegraphics{franke.pdf}
    \caption{The Franke's function to be fitted, given by~\vref{eq:franke}.}\label{fig:franke}
\end{figure}

\subsection{Fitting problem statement}
The general problem is to fit a given set of data \(D=\qty{(\vec{x}_i,y_i)}_{i=1}^N\), where \(\vec{x}_i\) are inputs of some dimension (\(2\) in this project) and \(y_i\) are scalar outputs.
A set of basis functions \(\qty{\phi_j}_{j=1}^p\) is chosen, and the goal is to approximate the input data with the linear combination \(\beta_j \phi_j(\vec{x})\).
If the data set were generated by such a linear combination, there would exist parameters \(\qty{\beta_j}\) such that \(\beta_j\phi_j(\vec{x}_i)=y_i\), which can be rewritten as the matrix equation
\begin{equation}
    X\vec{\beta} = \vec{y},
\end{equation}
where \(X\in\mathbb{R}^{N\times p}\) is the matrix with elements \(x_{ij}=\phi_j(x_i)\), and \(\vec{\beta}\) and \(\vec{y}\) are the vectors with elements \(\beta_j\) and \(y_i\).

In general, it will not be possible to find parameters \(\beta_j\) which fulfill this, since the data set will not be generated from the simple basis functions.
Consequently, one must find the \(\beta_j\)s which in some sense make \(X\vec{\beta}\) as close to \(\vec{y}\) as possible.
The way in which to measure deviation from the perfect solution is called the \emph{cost function}, frequently written \(Q(\vec{\beta};D)\).
Regression methods differ in the choice of cost function, while their aim is always to find the parameters \(\beta_j\) which minimise the cost function.

When a regression method has been used to find \(\vec{\beta}\), predicted values are denoted \(\tilde{y}_i = X_{ij}\beta_j\), while the original, exact values are denoted \(y_i\).

%                _ _                          _                _
%   ___  _ __ __| (_)_ __   __ _ _ __ _   _  | | ___  __ _ ___| |_
%  / _ \| '__/ _` | | '_ \ / _` | '__| | | | | |/ _ \/ _` / __| __|
% | (_) | | | (_| | | | | | (_| | |  | |_| | | |  __/ (_| \__ \ |_
%  \___/|_|  \__,_|_|_| |_|\__,_|_|   \__, | |_|\___|\__,_|___/\__|
%                                     |___/
%  ___  __ _ _   _  __ _ _ __ ___  ___
% / __|/ _` | | | |/ _` | '__/ _ \/ __|
% \__ \ (_| | |_| | (_| | | |  __/\__ \
% |___/\__, |\__,_|\__,_|_|  \___||___/
%         |_|
\subsection{Ordinary Least Squares}
The ordinary least squares method is the simplest and most intuitive, since its cost function is simply the square of the error made by the fit,
\begin{equation}
    Q(\beta;D) = \norm{\vec{y}-X\vec{\beta}}_2^2,
\end{equation}
where \(\norm{\cdot}_p\) denotes the usual \(p\)-norm.

\subsubsection{Geometric view}
The geometric view of the equation \(X\vec{\beta}=\vec{y}\) is to find the linear combination of the columns of \(X\) equal to \(\vec{y}\).
This is not necessarily possible if the columns of \(X\) do not span \(\mathbb{R}^N\), which is not possible if \(p<N\).
Ordinary least squares seeks to find the linear combination of the columns of \(X\) as close to \(\vec{y}\) as possible.
This is achieved when \(X\vec{\beta}\) is equal to the projection of \(\vec{y}\) onto the column space of \(X\), i.e.
\begin{equation}
    X\vec{\beta} = \Proj_{\Col{X}}{\vec{y}}.
\end{equation}
By construction, this equation will always have a solution, although it may not be unique if the columns of \(X\) are not linearly independent.
Since \(X\vec{\beta}\) is the projection of \(\vec{y}\) onto the column space of \(X\), the error, \(\vec{y}-X\vec{\beta}\), is orthogonal to the columns of \(X\), i.e.\ the rows of \(X^T\).
Consequently,
\begin{equation}
    X^T\qty(\vec{y}-X\vec{\beta}) = \vec{0}
    \implies
    X^T X \vec{\beta} = X^T \vec{y},\label{eq:olsnormal}
\end{equation}
which is a simple linear set of equations which can be solved for \(\vec{\beta}\).
This set of equations is called the \emph{normal equations}.
The minimisation problem in ordinary least squares therefore has a closed form solution,
\begin{equation}
    \vec{\beta}_{\text{OLS}} = \qty(X^T X)^{-1} X^T \vec{y}.
\end{equation}

\subsubsection{Minimisation view}
The cost function for ordinary least squares can be written as
\begin{equation}
    Q(\beta;D) = \norm{\vec{y}-X\vec{\beta}}_2^2
               = \sum_{i=1}^N \qty(y_i - X_{ij}\beta_j)^2.
\end{equation}
When this is minimised, \(\nabla Q = \vec{0}\), where the gradient denotes differentiation with respect to the parameters \(\beta_j\).
The components of the gradient can straightforwardly be calculated from the cost function,
\begin{equation}
    \nabla_k Q = \pdv{\beta_k}(\qty(y_i - X_{ij}\beta_j)\qty(y_i - X_{ij}\beta_j))
               = 2\qty(y_i-X_{ij}\beta_j) \pdv{\beta_k}(y_i-X_{ij}\beta_j)
               = -2X_{ik}\qty(y_i-X_{ij}\beta_j),\label{eq:olsgradcomp}
\end{equation}
which can be rewritten on vector form as
\begin{equation}
    \nabla Q = -2X^T\qty(\vec{y}-X\vec{\beta}).\label{eq:olsgrad}
\end{equation}
Setting \(\nabla Q = \vec{0}\) gives~\vref{eq:olsnormal}.

Ordinary least squares has been implemented by calling \lstinline{dgels}, which uses a QR-factorisation.

%      _     _
% _ __(_) __| | __ _  ___
%| '__| |/ _` |/ _` |/ _ \
%| |  | | (_| | (_| |  __/
%|_|  |_|\__,_|\__, |\___|
%              |___/
\subsection{Ridge regression}
The cost function used in Ridge regression is
\begin{equation}
    Q_\lambda(\vec{\beta};D) = \norm{\vec{y}-X\vec{\beta}}_2^2 + \lambda \norm{\vec{\beta}}_2^2
                             = \sum_{i=1}^N \qty(y_i - X_{ij}\beta_j)^2 + \lambda\sum_{i=1}^p \beta_i^2,
\end{equation}
which penalises solution vectors \(\vec{\beta}\) where some coefficients are large.
\(\lambda\) is a parameter which should be chosen with great care.
The error is as small as possible for \(\lambda=0\), as this will reproduce the solution found by ordinary least squares, but a non-zero value of \(\lambda\) will give \(\beta_j\)s which yield more reasonable predictions for other values of \(\vec{x}\) than those contained in the training data set \(D\)\cite{mehta}.

Using the derivative of the first term from~\vref{eq:olsgrad}, the gradient of the cost function is
\begin{equation}
    \nabla Q_\lambda = -2 X^T \qty(\vec{y}-X\vec{\beta}) + 2\lambda \vec{\beta}
                     = -2 X^T \vec{y} + 2 X^T X \vec{\beta} + 2\lambda \vec{\beta}
                     = -2 X^T \vec{y} + 2\qty(X^T X + \lambda I)\vec{\beta}.
\end{equation}
Minimisation requires \(\nabla Q_\lambda = \vec{0}\), which gives
\begin{equation}
    \qty(X^T X + \lambda I)\vec{\beta} = X^T \vec{y}.
\end{equation}
This can be solved for \(\vec{\beta}\), and the closed-form solution to the minimisation of the Ridge cost function is
\begin{equation}
    \vec{\beta}_\text{Ridge} = \qty(X^T X + \lambda I)^{-1} X^T \vec{y}.
\end{equation}
Having a non-zero \(\lambda\) clearly reduces problems with linearly dependent columns of \(X\) and singularity of \(X^T X\).

Ridge regression has been implemented by calculating \(X^T X\) and adding \(\lambda\) on the diagonal, calculating \(X^T y\) and using \lstinline{dposv} to find \(\vec{\beta}\) using a Cholesky-decomposition, since \(X^T X + \lambda I\) is positive definite.

% _
%| | __ _ ___ ___  ___
%| |/ _` / __/ __|/ _ \
%| | (_| \__ \__ \ (_) |
%|_|\__,_|___/___/\___/
\tikzexternaldisable
\subsection{LASSO regression}
The cost function used in LASSO regression is
\begin{equation}
    Q_\lambda(\vec{\beta};D) = \norm{\vec{y}-X\vec{\beta}}_2^2 + \lambda \norm{\vec{\beta}}_1
                             = \sum_{i=1}^N \qty(y_i - X_{ij}\beta_j)^2 + \lambda\sum_{i=1}^p \abs{\beta_i},
\end{equation}
which also penalises solution vectors \(\vec{\beta}\) where some coefficients are large.
LASSO regression has the advantage that certain choices of \(\lambda\) will give a sparse solution, i.e. \(\beta_j=0\) for some values of \(j\)\cite{wieringen}.

As with the other regression methods, the gradient of the cost function should now be differentiated and set to zero.
Mathematicians will now point out that the absolute value is not differentiable at zero and start deriving ingenious minimisation methods to circumvent the problem.
This is a non-issue in a physics course --- I will now \emph{define} the derivative to be
\begin{equation}
    \dv{\abs{x}}{x} \coloneqq \sgn{x} \coloneqq \begin{cases}1, & x > 0\\ 0, & x = 0 \\ -1, & x < 0 \end{cases}
\end{equation}
and proceed happily.

The gradient of the cost function is thus
\begin{equation}
    \nabla Q_\lambda = -2 X^T \qty(\vec{y} - X\vec{\beta}) + \lambda \sgn\qty(\vec{\beta}),
\end{equation}
and \(\nabla Q_\lambda = \vec{0}\) does unfortunately not have a closed-form solution.
A separate minimisation algorithm must therefore be used to find the parameters \(\beta_j\) which minimise the cost function.
The calculated gradient can be put straight into the gradient descent algorithm with a suitable step length.
Alternatively, Newton's method can be used with the second derivative (Hessian) matrix of the cost function.
The second derivative of the absolute value is even more problematic, which is solved by setting it equal to zero.

The Hessian matrix of the LASSO cost function is thus the same as the second derivative of the ordinary least squares cost function,
\begin{equation}
    H_{kl} = H_{lk} = \pdv{Q_\lambda}{\beta_l}{\beta_k}
           = \pdv{\beta_l}(\nabla_k Q)
           \,\husk{gradref}{=}\, \pdv{\beta_l}(-2X_{ik}\qty(y_i-X_{ij}\beta_j))
           = 2X_{ik}X_{il},
\tikz[>=latex,overlay,remember picture,thick]{\draw[<-,gronn] (gradref) .. controls ++(0,0.75) and ++(-1,0) .. ++(1,0.75) node[anchor=west] {\small\ref{eq:olsgradcomp}};}
\end{equation}
which is the component form of the matrix equation
\begin{equation}
    H = 2 X^T X.
\end{equation}
The gradient can now be rewritten as
\begin{equation}
    \nabla Q_\lambda = -2 X^T \vec{y} + H\vec{\beta} + \lambda \sgn\qty(\vec{\beta}).
\end{equation}
\Vref*{eq:newtonstep} in Newton's method can then transformed into
\begin{equation}
    H\qty(\vec{\beta}_{i+1} - \vec{\beta}_i) = 2X^T \vec{y}  - H\vec{\beta}_i - \lambda \sgn\qty(\vec{\beta}_i)
    \implies H\vec{\beta}_{i+1} = 2X^T\vec{y} - \lambda \sgn\qty(\vec{\beta}_i). \label{eq:lassonewton}
\end{equation}
Since \(H=2X^T X\), this reduces to the normal equations of ordinary least squares (\vref*{eq:olsnormal}) in the case of \(\lambda=0\), as it should.

LASSO regression has been implemented by calculating the Cholesky-decomposition of \(H\) using \lstinline{dpptrf}, guessing \(\beta_j=1\) and then repeatedly solving~\vref{eq:lassonewton} using \lstinline{dpptrs} until convergence, as illustrated by the following snippet.
\lstinputlisting[linerange={lassostart-lassoend}]{lib/lasso2d.f90}

%           _       _           _           _   _
% _ __ ___ (_)_ __ (_)_ __ ___ (_)___  __ _| |_(_) ___  _ __
%| '_ ` _ \| | '_ \| | '_ ` _ \| / __|/ _` | __| |/ _ \| '_ \
%| | | | | | | | | | | | | | | | \__ \ (_| | |_| | (_) | | | |
%|_| |_| |_|_|_| |_|_|_| |_| |_|_|___/\__,_|\__|_|\___/|_| |_|
\subsection{Minimisation methods}
The important step in a regression method, or indeed in any statistical learning method, is to minimise the cost function.
Certain cost functions, such as those of ordinary least squares and Ridge regression, admit analytical closed-form solutions for the parameters \(\beta_j\) which minimise \(Q(\vec{\beta};D)\), while most methods, including LASSO regression and logistic regression, require usage of some general minimisation algorithm.

\subsubsection{Newton's method}\label{subsubsec:newton}
Since the gradient of the cost function, \(\nabla Q\), is a perfectly normal function from \(\mathbb{R}^p\) to \(\mathbb{R}^p\), it can be Taylor expanded around some point \(\vec{\beta}_0\).
This Taylor expansion can then be evaluated at a point \(\vec{\beta}_0 + \delta\vec{\beta}\). To first order in \(\delta\vec{\beta}\),
\begin{equation}
    \nabla Q\qty(\vec{\beta}_0 + \delta\vec{\beta}) = \nabla Q\qty(\vec{\beta}_0) + H\qty(\vec{\beta}_0)\delta\vec{\beta},
\end{equation}
where \(H\qty(\vec{\beta}_0)\) is the derivative of \(\nabla Q\), i.e.\ the Hessian of \(Q\), evaluated at \(\beta_0\).
Given a guess \(\beta_0\) somewhere near the minimum of \(Q\), a better estimate can be found by solving for the \(\delta\vec{\beta}\) which makes \(\nabla Q(\vec{\beta}_0 + \delta\vec{\beta}) = \vec{0}\), i.e.
\begin{equation}
    H\qty(\vec{\beta}_0)\delta\vec{\beta} = -\nabla Q\qty(\vec{\beta}_0),
\end{equation}
and, in general, solving
\begin{equation}
    H\qty(\vec{\beta}_i)\delta\vec{\beta}_i = -\nabla Q\qty(\vec{\beta}_i),\label{eq:newtonstep}
\end{equation}
letting \(\vec{\beta}_{i+1} = \vec{\beta}_i + \delta\vec{\beta}_i\) and continuing the process until the method has converged sufficiently.
The above equation is a simple linear set of equations.

Minimisation of the LASSO cost function has the benefit that the second derivative is independent of \(\vec{\beta}\).
\(H\) can therefore be Cholesky-decomposed once, an \(\mathcal{O}\qty(n^3)\) operation, and the result can be used to solve the linear set of equations for each iteration, which is \(\mathcal{O}\qty(n^2)\) with a pre-Cholesky-decomposed matrix.
The Cholesky-decomposition can be used instead of an LU-decomposition, reducing the number of floating point operations by a factor of two, because \(H=2X^T X\) is positive definite when \(X\) is non-singular.

Evaluation of the gradient only involves matrix-vector products, which is also an \(\mathcal{O}\qty(n^2)\) operation.
The minimisation can, therefore, be done with one initial \(\mathcal{O}\qty(n^3)\) and then only \(\mathcal{O}\qty(n^2)\) operations per iteration, while the more general problem requires both the construction and the Cholesky-decomposition of \(H\), an \(\mathcal{O}\qty(n^3)\) operation, for every single iteration.

\subsubsection{Gradient descent}
Evaluating the Hessian matrix and inverting it can sometimes be infeasible, for example due to poor time complexity or being woefully undefined.
In such cases, one can replace the Hessian \(H\) with a number \(1/\alpha\).
\Vref{eq:newtonstep} is then rewritten as
\begin{equation}
    \vec{\beta}_{i+1} = \vec{\beta_i} - \alpha \nabla Q\qty(\vec{\beta}_i),
\end{equation}
with a simple geometric interpretation:
By going a small step in the opposite direction of the gradient, the value of the cost function will decrease and \(\vec{\beta}\) will approach the true minimum.
A small step will guarantee convergence for a convex function at the cost of slow convergence, while a larger value may give faster convergence or not converge at all.

%                   __
%  _ __   ___ _ __ / _| ___  _ __ _ __ ___   __ _ _ __   ___ ___
% | '_ \ / _ \ '__| |_ / _ \| '__| '_ ` _ \ / _` | '_ \ / __/ _ \
% | |_) |  __/ |  |  _| (_) | |  | | | | | | (_| | | | | (_|  __/
% | .__/ \___|_|  |_|  \___/|_|  |_| |_| |_|\__,_|_| |_|\___\___|
% |_|
\subsection{Performance of regression methods}
While the cost function is minimised by all regression methods, its value is not particularly meaningful.
In particular, the cost functions in Ridge and LASSO regression depend on the parameter \(\lambda\), and a measure independent of \(\lambda\) should be used to determine the optimal \(\lambda\).
Other functions are therefore introduced to measure the performance of a given regression method and/or a choice of parameters.

The simplest measure is the mean square error,
\begin{equation}
    \MSE\qty(\vec{\beta},D) = \frac{1}{N}\norm{\vec{y}-\vec{\tilde{y}}}_2^2 = \frac{1}{N} \sum_{i=1}^N \qty(y_i - \tilde{y}_i)^2,
\end{equation}
which should be as small as possible. Another measure is the \(R^2\) score, defined as
\begin{equation}
    R^2(\vec{\beta},D) = 1 - \frac{\norm{\vec{y} - \vec{\tilde{y}}}_2^2}{\norm{\vec{y} - \bar{y}}_2^2}
                       = 1 - \frac{\sum_{i=1}^N \qty(y_i - \tilde{y}_i)^2}{\sum_{i=1}^N \qty(y_i - \bar{y})^2},
\end{equation}
where \(\bar{y}\) is the mean of the measured values. The \(R^2\) score should be as close to \(1\) as possible.

Prediction and these measures of performance can be applied to two main types of data.
Firstly, it can be applied to the data from which \(\vec{\beta}\) was derived, which is called the training data.
Ordinary least squares, corresponding to \(\lambda=0\) for the other methods, will, by definition, give the best results for this data set.
Secondly, the performance can be measured for values not among the training data, called test data. Ridge and LASSO are expected to outperform ordinary least squares for small, non-zero values of \(\lambda\) for this category of data.

%                                      _ _
%  _ __ ___  ___  __ _ _ __ ___  _ __ | (_)_ __   __ _
% | '__/ _ \/ __|/ _` | '_ ` _ \| '_ \| | | '_ \ / _` |
% | | |  __/\__ \ (_| | | | | | | |_) | | | | | | (_| |
% |_|  \___||___/\__,_|_| |_| |_| .__/|_|_|_| |_|\__, |
%                               |_|              |___/
\subsection{Resampling methods}
Resampling methods are techniques to improve the prediction accuracy and obtain estimates for quantities such as the variance of \(\vec{\beta}\) by repeatedly dividing the data set into training and test data.
Two simple examples are \(k\)-fold cross-validation and bootstrapping.
The former partitions the data set into \(k\) partitions.
One of these subsets is chosen as test data on which the performance is measured, while the rest are used as training data.
This process is the repeated \(k\) times, so that each subset is used once as test data.
Bootstrapping, on the other hand, repeatedly creates training data sets by randomly selecting \(N\) values from the data set with replacement, while the original data set is used as test data.

%  _     _                             _
% | |__ (_) __ _ ___    __ _ _ __   __| |
% | '_ \| |/ _` / __|  / _` | '_ \ / _` |
% | |_) | | (_| \__ \ | (_| | | | | (_| |
% |_.__/|_|\__,_|___/  \__,_|_| |_|\__,_|
%
%                  _
% __   ____ _ _ __(_) __ _ _ __   ___ ___
% \ \ / / _` | '__| |/ _` | '_ \ / __/ _ \
%  \ V / (_| | |  | | (_| | | | | (_|  __/
%   \_/ \__,_|_|  |_|\__,_|_| |_|\___\___|
\subsection{Bias and variance}
The choice of the number of basis functions, \(p\), determines the complexity of the model to which the data set is fitted.
A higher complexity will mean that the model will be a better approximation to the training data, which is said to reduce the \emph{bias} of the model.
On the other hand, a higher complexity may decrease the model's ability to predict reasonable values for test data, since a more complex model will be more affected by noise in the model.
This is said to increase the model's variance.
The best prediction performance is therefore achieved for a complexity which balances bias and variance, which is called the bias-variance trade-off\cite{mehta}.

Following~\cite{mehta}, a mathematical manifestation of the bias-variance trade-off can be derived by assuming that the measured data set \(D=\qty{\qty(\vec{x}_i,y_i)}_{i=1}^N\) is generated by a combination of some model \(f\qty(\vec{x})\) (for example Franke's function) and random noise, specifically
\begin{equation}
    y_i = f\qty(\vec{x}_i) + \varepsilon_i \eqqcolon f_i + \varepsilon_i,
\end{equation}
where the variables \(\varepsilon_i\) are independent and normally distributed with variance \(\sigma^2\) centred in zero.
Given a data set \(D\), a prediction \(\tilde{y}_i^D\) can be made by any of the regression methods discussed above.
An expected mean square error can be found by averaging over all datasets \(D\) and all noises \(\vec{\varepsilon}\),
\begin{alignat}{2}
        E_{D,\varepsilon}\qty[\norm{\vec{y}-\vec{\tilde{y}}_D}_2^2]
        &= E_{D,\varepsilon}\qty[\norm{\vec{y} - \vec{f} + \vec{f} - \vec{\tilde{y}}_D}_2^2]
         = E_{D,\varepsilon}\qty[\norm{\vec{y} - \vec{f}}_2^2 + \norm{\vec{f} - \vec{\tilde{y}}_D}_2^2
                                 + 2 \qty(\vec{y} - \vec{f})\cdot\qty(\vec{f} - \vec{\tilde{y}}_D)],
        \shortintertext{and using the linearity of the expectation value,}
        &= E_{D,\varepsilon}\qty[\norm{\vec{y} - \vec{f}}_2^2] + E_{D,\varepsilon}\qty[\norm{\vec{f} - \vec{\tilde{y}}_D}_2^2]
                                 + 2 E_{D,\varepsilon}\qty[\qty(\vec{y} - \vec{f})\cdot\qty(\vec{f} - \vec{\tilde{y}}_D)]\\
        &= E_{D,\varepsilon}\qty[\norm{\vec{\varepsilon}}_2^2] + E_{D,\varepsilon}\qty[\norm{\vec{f} - \vec{\tilde{y}}_D}_2^2]
           + 2 E_{D,\varepsilon}\qty[\vec{\varepsilon}\cdot\qty(\vec{f} - \vec{\tilde{y}}_D)]\\
        &= \sigma^2 + E_{D,\varepsilon}\qty[\norm{\vec{f} - \vec{\tilde{y}}_D}_2^2]
           + 2 \cancel{E_{\varepsilon}\qty[\vec{\varepsilon}]}\cdot E_{D}\qty[\qty(\vec{f} - \vec{\tilde{y}}_D)]\\
        &= \sigma^2 + E_{D,\varepsilon}\qty[\norm{\vec{f} - \vec{\tilde{y}}_D}_2^2].
\end{alignat}











\clearpage
\nocite{*}
\printbibliography{}
\addcontentsline{toc}{section}{\bibname}
\end{document}
