\documentclass[11pt,british,a4paper]{article}
%\pdfobjcompresslevel=0
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[includeheadfoot,margin=0.8 in]{geometry}
\usepackage{siunitx,physics,cancel,upgreek,varioref,listings,booktabs,pdfpages,ifthen,polynom,todonotes}
%\usepackage{minted}
\usepackage[backend=biber]{biblatex}
\DefineBibliographyStrings{english}{%
      bibliography = {References},
}
\addbibresource{sources.bib}
\usepackage{mathtools,upgreek,bigints}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb,epstopdf}
\usepackage[T1]{fontenc}
%\usepackage{fouriernc}
% \usepackage[T1]{fontenc}
\usepackage{mathpazo}
% \usepackage{inconsolata}
%\usepackage{eulervm}
%\usepackage{cmbright}
%\usepackage{fontspec}
%\usepackage{unicode-math}
%\setmainfont{Tex Gyre Pagella}
%\setmathfont{Tex Gyre Pagella Math}
%\setmonofont{Tex Gyre Cursor}
%\renewcommand*\ttdefault{txtt}
\graphicspath{{figs/}}
\usepackage[scaled]{beramono}
\usepackage{fancyhdr}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{lastpage}
\usepackage{microtype}
\usepackage[font=normalsize]{subcaption}
\usepackage{luacode}
\usepackage[linktoc=all, bookmarks=true, pdfauthor={Anders Johansson},pdftitle={FYS-STK4155 Project 1}]{hyperref}
\usepackage{tikz,pgfplots,pgfplotstable}
\usepgfplotslibrary{colorbrewer}
\usepgfplotslibrary{external}
\tikzset{external/system call={lualatex \tikzexternalcheckshellescape -halt-on-error -interaction=batchmode -jobname "\image" "\texsource"}}
\tikzexternalize[prefix=tmp/, mode=list and make]
\pgfplotsset{cycle list/Dark2}
\pgfplotsset{compat=1.8}
\renewcommand{\CancelColor}{\color{red}}
\let\oldexp=\exp
\renewcommand{\exp}[1]{\mathrm{e}^{#1}}
\renewcommand{\Re}[1]{\mathfrak{Re}\ifthenelse{\equal{#1}{}}{}{\left(#1\right)}}
\renewcommand{\Im}[1]{\mathfrak{Im}\ifthenelse{\equal{#1}{}}{}{\left(#1\right)}}
\renewcommand{\i}{\mathrm{i}}
\newcommand{\tittel}[1]{\title{#1 \vspace{-7ex}}\author{}\date{}\maketitle\thispagestyle{fancy}\pagestyle{fancy}\setcounter{page}{1}}


\labelformat{section}{#1}
\labelformat{subsection}{exercise~#1}
\labelformat{subsubsection}{paragraph~#1}
\labelformat{equation}{equation~(#1)}
\labelformat{figure}{figure~#1}
\labelformat{table}{table~#1}

\renewcommand{\footrulewidth}{\headrulewidth}

%\setcounter{secnumdepth}{4}
\setlength{\parindent}{0cm}
\setlength{\parskip}{1em}

\definecolor{bluekeywords}{rgb}{0.13,0.13,1}
\definecolor{greencomments}{rgb}{0,0.5,0}
\definecolor{redstrings}{rgb}{0.9,0,0}
\lstset{rangeprefix=!/,
    rangesuffix=/!,
    includerangemarker=false}
\lstset{showstringspaces=false,
    basicstyle=\small\ttfamily,
    keywordstyle=\color{bluekeywords},
    commentstyle=\color{greencomments},
    numberstyle=\color{bluekeywords},
    stringstyle=\color{redstrings},
    breaklines=true,
    %texcl=true,
    language=Fortran
}
\colorlet{DarkGrey}{white!20!black}
\newcommand{\eqtag}[1]{\refstepcounter{equation}\tag{\theequation}\label{#1}}
\hypersetup{hidelinks=True}

\sisetup{detect-all}
\sisetup{exponent-product = \cdot, output-product = \cdot,per-mode=symbol}
% \sisetup{output-decimal-marker={,}}
\sisetup{round-mode = off, round-precision=3}
\sisetup{number-unit-product = \ }

\allowdisplaybreaks[4]
\fancyhf{}

\rhead{Project 1}
\rfoot{Page~\thepage{} of~\pageref{LastPage}}
\lhead{FYS-STK4155}

%\definecolor{gronn}{rgb}{0.29, 0.33, 0.13}
\definecolor{gronn}{rgb}{0, 0.5, 0}

\newcommand{\husk}[2]{\tikz[baseline,remember picture,inner sep=0pt,outer sep=0pt]{\node[anchor=base] (#1) {\(#2\)};}}
\newcommand{\artanh}[1]{\operatorname{artanh}{\qty(#1)}}
\newcommand{\matrise}[1]{\begin{pmatrix}#1\end{pmatrix}}
\DeclareMathOperator{\Proj}{Proj}
\DeclareMathOperator{\Col}{Col}

\newread\infile

\setcounter{tocdepth}{2}

%start
\begin{document}
\title{FYS-STK4155: Project 1}
\author{Anders Johansson}
%\maketitle

\begin{titlepage}
%\includegraphics[width=\textwidth]{fysisk.pdf}
\vspace*{\fill}
\begin{center}
\textsf{
    \Huge \textbf{Project 1}\\\vspace{0.5cm}
    \Large \textbf{FYS-STK4155 --- Applied data analysis and machine learning}\\
    \vspace{8cm}
    Anders Johansson\\
    \today\\
}
\vspace{1.5cm}
\includegraphics{uio.pdf}\\
\vspace*{\fill}
\end{center}
\end{titlepage}
\null
\pagestyle{empty}
\newpage

\pagestyle{fancy}
\setcounter{page}{1}

\begin{abstract}
    This project uses fitting of Franke's function and geographical data to compare ordinary least squares, Ridge and LASSO regression. Resampling is done in order to get accurate estimates for variances and means.
\end{abstract}

All files for this project are available at \url{https://github.com/anjohan/ml1}.

\tableofcontents

%  _       _
% (_)_ __ | |_ _ __ ___
% | | '_ \| __| '__/ _ \
% | | | | | |_| | | (_) |
% |_|_| |_|\__|_|  \___/
\section{Introduction}
Fitting of data is an important tool in most sciences.
The goal can be to interpolate between already measured data or predict values outside the measured range when extra experiments are infeasible, or to discover relations between quantities.
By assuming that the data points can be modelled by a linear combination of some set of functions and minimising the error, one ends up with linear regression.
This report explores three different linear regression methods --- ordinary least squares, which simply minimises the error, and Ridge and LASSO, which make up for their worse performance on training data by returning better predictors.

The main goal of this project is to compare these three regression methods empirically by applying it to two sets of data.
First, the regression methods are applied to a data set generated from a known function, specifically the Franke's function, which is a sum of bivariate Gaussian functions.
Having verified the implementation of the methods, they are then applied to geographic data, namely the altitude as a function of geographic coordinates.

The first part of this report goes through the basic theory of these regression methods and their implementation.
Then, the regression methods are applied to the Franke's function and the results are analysed with resampling. Finally, geographical data is fitted with all three methods.

%  _   _
% | |_| |__   ___  ___  _ __ _   _
% | __| '_ \ / _ \/ _ \| '__| | | |
% | |_| | | |  __/ (_) | |  | |_| |
%  \__|_| |_|\___|\___/|_|   \__, |
%                            |___/
\section{Theory and methods}

\subsection{Test case}
The function to be fitted as verification of the regression methods is the famous Franke's function \(f:\qty[0,1]^2\to\mathbb{R}\) given by
\begin{equation}
    \begin{alignedat}{2}
        f(x,y) &= \frac{3}{4}\oldexp(-\frac{(9x-2)^2}{4} - \frac{(9y-2)^2}{4})
                + \frac{3}{4}\oldexp(-\frac{(9x+1)^2}{49}- \frac{(9y+1)}{10} ) \\
               &+ \frac{1}{2}\oldexp(-\frac{(9x-7)^2}{4} - \frac{(9y-3)^2}{4})
                - \frac{1}{5}\oldexp(-(9x-4)^2 - (9y-7)^2)
    \end{alignedat}\label{eq:franke}
\end{equation}
and shown in~\vref{fig:franke}.

\begin{figure}[H]
    \centering
    \includegraphics{franke.pdf}
    \caption{The Franke's function to be fitted, given by~\vref{eq:franke}.}\label{fig:franke}
\end{figure}

\subsection{Fitting problem statement}
The general problem is to fit a given set of data \(D=\qty{(\vec{x}_i,y_i)}_{i=1}^N\), where \(\vec{x}_i\) are inputs of some dimension (\(2\) in this project) and \(y_i\) are scalar outputs.
A set of basis functions \(\qty{\phi_j}_{j=1}^p\) is chosen, and the goal is to approximate the input data with the linear combination \(\beta_j \phi_j(\vec{x})\).
If the data set were generated by such a linear combination, there would exist parameters \(\qty{\beta_j}\) such that \(\beta_j\phi_j(\vec{x}_i)=y_i\), which can be rewritten as the matrix equation
\begin{equation}
    X\vec{\beta} = \vec{y},
\end{equation}
where \(X\in\mathbb{R}^{N\times p}\) is the matrix with elements \(x_{ij}=\phi_j(x_i)\), and \(\vec{\beta}\) and \(\vec{y}\) are the vectors with elements \(\beta_j\) and \(y_i\).

In general, it will not be possible to find parameters \(\beta_j\) which fulfill this, since the data set will not be generated from the simple basis functions.
Consequently, one must find the \(\beta_j\)s which in some sense make \(X\vec{\beta}\) as close to \(\vec{y}\) as possible.
The way in which to measure deviation from the perfect solution is called the \emph{cost function}, frequently written \(Q(\vec{\beta};D)\).
Regression methods differ in the choice of cost function, while their aim is always to find the parameters \(\beta_j\) which minimise the cost function.

%                _ _                          _                _
%   ___  _ __ __| (_)_ __   __ _ _ __ _   _  | | ___  __ _ ___| |_
%  / _ \| '__/ _` | | '_ \ / _` | '__| | | | | |/ _ \/ _` / __| __|
% | (_) | | | (_| | | | | | (_| | |  | |_| | | |  __/ (_| \__ \ |_
%  \___/|_|  \__,_|_|_| |_|\__,_|_|   \__, | |_|\___|\__,_|___/\__|
%                                     |___/
%  ___  __ _ _   _  __ _ _ __ ___  ___
% / __|/ _` | | | |/ _` | '__/ _ \/ __|
% \__ \ (_| | |_| | (_| | | |  __/\__ \
% |___/\__, |\__,_|\__,_|_|  \___||___/
%         |_|
\subsection{Ordinary Least Squares (OLS)}
The ordinary least squares method is the simplest and most intuitive, since its cost function is simply the square of the error made by the fit,
\begin{equation}
    Q(\beta;D) = \norm{\vec{y}-X\vec{\beta}}_2^2,
\end{equation}
where \(\norm{\cdot}_p\) denotes the usual \(p\)-norm.

\subsubsection{Geometric view}
The geometric view of the equation \(X\vec{\beta}=\vec{y}\) is to find the linear combination of the columns of \(X\) equal to \(\vec{y}\).
This is not necessarily possible if the columns of \(X\) do not span \(\mathbb{R}^N\), which is not possible if \(p<N\).
Ordinary least squares seeks to find the linear combination of the columns of \(X\) as close to \(\vec{y}\) as possible.
This is achieved when \(X\vec{\beta}\) is equal to the projection of \(\vec{y}\) onto the column space of \(X\), i.e.
\begin{equation}
    X\vec{\beta} = \Proj_{\Col{X}}{\vec{y}}.
\end{equation}
By construction, this equation will always have a solution, although it may not be unique if the columns of \(X\) are not linearly independent.
Since \(X\vec{\beta}\) is the projection of \(\vec{y}\) onto the column space of \(X\), the error, \(\vec{y}-X\vec{\beta}\), is orthogonal to the columns of \(X\), i.e.\ the rows of \(X^T\).
Consequently,
\begin{equation}
    X^T\qty(\vec{y}-X\vec{\beta}) = \vec{0}
    \implies
    X^T X \vec{\beta} = X^T \vec{y},\label{eq:olsnormal}
\end{equation}
which is a simple linear set of equations which can be solved for \(\vec{\beta}\).
This set of equations is called the \emph{normal equations}.
The minimisation problem in ordinary least squares therefore has a closed form solution,
\begin{equation}
    \vec{\beta}_{\text{OLS}} = \qty(X^T X)^{-1} X^T \vec{y}.
\end{equation}

\subsubsection{Minimisation view}
The cost function for ordinary least squares can be written as
\begin{equation}
    Q(\beta;D) = \norm{\vec{y}-X\vec{\beta}}_2^2
               = \sum_{i=1}^N \qty(y_i - X_{ij}\beta_j)^2.
\end{equation}
When this is minimised, \(\nabla Q = \vec{0}\), where the gradient denotes differentiation with respect to the parameters \(\beta_j\).
The components of the gradient can straightforwardly be calculated from the cost function,
\begin{equation}
    \nabla_k Q = \pdv{\beta_k}(\qty(y_i - X_{ij}\beta_j)\qty(y_i - X_{ij}\beta_j))
               = 2\qty(y_i-X_{ij}\beta_j) \pdv{\beta_k}(y_i-X_{ij}\beta_j)
               = -2X_{ik}\qty(y_i-X_{ij}\beta_j),
\end{equation}
which can be rewritten on vector form as
\begin{equation}
    \nabla Q = -2X^T\qty(\vec{y}-X\vec{\beta}).
\end{equation}
Setting \(\nabla Q = \vec{0}\) gives~\vref{eq:olsnormal}.

%      _     _
% _ __(_) __| | __ _  ___
%| '__| |/ _` |/ _` |/ _ \
%| |  | | (_| | (_| |  __/
%|_|  |_|\__,_|\__, |\___|
%              |___/
\subsection{Ridge regression}





\clearpage
\nocite{*}
\printbibliography{}
\addcontentsline{toc}{section}{\bibname}
\end{document}
